
EXPERIMENT 1 – FIND-S

students.csv

ex,PG,year,CGPA,sports,interest,scholarship
1,MCA,2,high,yes,Dance,yes
2,MCA,2,high,no,Dance,yes
3,MCA,1,high,yes,Music,no
4,MBA,2,high,yes,Dance,no
5,MCA,2,medium,yes,Dance,yes

Code:

import pandas as pd
df = pd.read_csv("students.csv")
pos = df[df['scholarship'].str.lower() == 'yes']
hypothesis = list(pos.iloc[0, :-1])
for i, row in pos.iterrows():
    example = list(row[:-1])
    for j in range(len(hypothesis)):
        if hypothesis[j] != example[j]:
            hypothesis[j] = '?'
print("Final hypothesis:", hypothesis)


EXPERIMENT 2 – Candidate Elimination

students.csv

ex,PG,year,CGPA,sports,interest,scholarship
1,MCA,2,high,yes,Dance,yes
2,MCA,2,high,no,Dance,yes
3,MCA,1,high,yes,Music,no
4,MBA,2,high,yes,Dance,no
5,MCA,2,medium,yes,Dance,yes


Code:

import pandas as pd
df = pd.read_csv("students.csv")
attributes = list(df.columns[:-1])
target = 'scholarship'
S = [['ϕ'] * len(attributes)]
G = [['?' for _ in range(len(attributes))]]
def more_general(h1, h2):
    more = False
    for x, y in zip(h1, h2):
        if x != '?' and (y == 'ϕ' or (x != y and y != '?')):
            return False
        if x == '?' and y != '?':
            more = True
    return more or h1 == h2
for i, row in df.iterrows():
    example = list(row[:-1])
    label = row[target]
    if label == 'yes':
        if S[0][0] == 'ϕ':
            S[0] = example
        for g in G[:]:
            if not more_general(g, example):
                G.remove(g)
        for j in range(len(S[0])):
            if S[0][j] != example[j]:
                S[0][j] = '?'
    else:
        G_new = []
        for g in G:
            if all(x == '?' or x == y for x, y in zip(g, example)):
                for j in range(len(g)):
                    if g[j] == '?':
                        if S[0][j] != example[j] and S[0][j] != '?':
                            h = g.copy()
                            h[j] = S[0][j]
                            if all(more_general(gh, S[0]) for gh in [h]):
                                G_new.append(h)
            else:
                G_new.append(g)
        G = []
        for h in G_new:
            if not any(more_general(h2, h) for h2 in G_new if h2 != h):
                G.append(h)
print("S:", S)
print("G:", G)


EXPERIMENT 3 – ID3 Decision Tree

data_id3.csv

CGPA,Sports,Projects,Attendance,Scholarship
1,1,1,1,yes
1,0,1,1,yes
1,0,0,1,no
0,1,1,1,yes
0,1,0,0,no
1,1,1,0,yes
0,0,1,1,no
1,1,0,1,yes
0,1,1,0,no
1,0,1,0,no

Code:

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, export_text
df = pd.read_csv("data_id3.csv")
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
model = DecisionTreeClassifier(criterion="entropy")
model.fit(X, y)
print(export_text(model, feature_names=list(X.columns)))
new_sample = [[1,2,1,0]]
print("Prediction:", model.predict(new_sample))

EXPERIMENT 4 – KNN (Iris)

Code:

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print("Correct:", (y_pred == y_test).sum())
print("Wrong:", (y_pred != y_test).sum())
print("Accuracy:", accuracy_score(y_test, y_pred))


EXPERIMENT 5 – ANN Backpropagation (MLP)

Code:

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

mlp = MLPClassifier(hidden_layer_sizes=(5,), solver='lbfgs', max_iter=1000, random_state=0)
mlp.fit(X_train, y_train)

y_pred = mlp.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))


EXPERIMENT 6 – Naive Bayes (CSV)

nb_data.csv

Feature1,Feature2,Class
1,1,yes
1,0,no
0,1,yes
0,0,no

Code:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
df = pd.read_csv("nb_data.csv")
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

EXPERIMENT 7 – Bayesian Network (Heart)

heart.csv

Age,Cholesterol,BP,HeartDisease
1,1,1,yes
1,1,0,yes
1,0,1,yes
1,0,0,no
0,1,1,yes
0,1,0,no
0,0,1,no
0,0,0,no
1,1,1,yes
0,1,1,yes

Code:
-----
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination
data = pd.read_csv("heart.csv")
model = BayesianNetwork([
    ('Age', 'HeartDisease'),
    ('Cholesterol', 'HeartDisease'),
    ('BP', 'HeartDisease')
])
model.fit(data, estimator=MaximumLikelihoodEstimator)
infer = VariableElimination(model)
q = infer.query(variables=['HeartDisease'],
                evidence={'Age':1,'Cholesterol':1,'BP':0})
print(q)


EXPERIMENT 8 – Naive Bayes Text Classification

Code:

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
docs=[
"this book is good",
"this movie is bad",
"i love this film",
"i hate this book",
"good story and good acting",
"worst film ever"
]
labels=['pos','neg','pos','neg','pos','neg']
vec=TfidfVectorizer()
X=vec.fit_transform(docs)
X_train,X_test,y_train,y_test=train_test_split(X,labels,test_size=0.3,random_state=0)
clf=MultinomialNB()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
print("Accuracy:",accuracy_score(y_test,y_pred))
test_doc=vec.transform(["the story is good"])
print("Class:",clf.predict(test_doc)[0])

EXPERIMENT 9 – EM (GMM) vs KMeans

Code:

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
X,_=make_blobs(n_samples=200,centers=3,random_state=0)
kmeans=KMeans(n_clusters=3,random_state=0)
kmeans_labels=kmeans.fit_predict(X)
gmm=GaussianMixture(n_components=3,random_state=0)
gmm_labels=gmm.fit_predict(X)
print("Silhouette KMeans:",silhouette_score(X,kmeans_labels))
print("Silhouette GMM:",silhouette_score(X,gmm_labels))

EXPERIMENT 10 – LWR

Code:
import numpy as np
import matplotlib.pyplot as plt

def lwlr(x_train, y_train, x_query, tau):
    y_pred = []
    for x0 in x_query:
        w = np.exp(- (x_train - x0)**2 / (2 * tau**2))
        W = np.diag(w)
        X = np.c_[np.ones(len(x_train)), x_train]
        theta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y_train)
        y_pred.append([1, x0] @ theta)
    return np.array(y_pred)

x_train = np.array([1,2,3,4,5], float)
y_train = np.array([1,4,9,16,25], float)

x_query = np.linspace(1,5,100)
y_query = lwlr(x_train, y_train, x_query, tau=0.5)

plt.scatter(x_train, y_train)
plt.plot(x_query, y_query)
plt.show()


